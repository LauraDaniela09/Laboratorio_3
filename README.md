# Lğšğ›ğ¨ğ«ğšğ­ğ¨ğ«ğ¢ğ¨ ğŸ‘ - Fğ«ğğœğ®ğğ§ğœğ¢ğš ğ² Vğ¨ğ³
ğ™ğ™£ğ™©ğ™§ğ™¤ğ™™ğ™ªğ™˜ğ™˜ğ™Ã³ğ™£

En este laboratorio se realizÃ³ un anÃ¡lisis espectral de la voz  empleando tÃ©cnicas de procesamiento digital de seÃ±ales. Se utilizaron grabaciones de voces masculinas y femeninas para calcular parÃ¡metros como la frecuencia fundamental, frecuencia media, brillo e intensidad.

ğ™¤ğ™—ğ™Ÿğ™šğ™©ğ™ğ™«ğ™¤

Aplicar la Transformada de Fourier para analizar y comparar las caracterÃ­sticas espectrales de voces masculinas y femeninas, identificando sus diferencias en frecuencia e intensidad para comprender mejor el comportamiento acÃºstico de la voz humana.

ğ™ğ™¢ğ™¥ğ™¤ğ™§ğ™©ğ™–ğ™˜ğ™Ã³ğ™£ ğ™™ğ™š ğ™¡ğ™ğ™—ğ™§ğ™šğ™§ğ™ğ™–ğ™¨
```python
import scipy.io.wavfile as wav
import matplotlib.pyplot as plt
import numpy as np
from IPython.display import Audio
from scipy.signal import find_peaks
from scipy.io import wavfile
```
Esa parte del cÃ³digo muestra la importaciÃ³n de librerÃ­as necesarias para trabajar con archivos de audio y analizarlos:`scipy.io.wavfile` y `wavfile` para leer y escribir archivos de audio `(.wav)`.`matplotlib.pyplot` para graficar seÃ±ales.`numpy` para realizar operaciones numÃ©ricas y de matrices. `IPython.display.Audio` para reproducir el audio directamente en el notebook.`scipy.signal.find_peaks`para detectar picos o puntos importantes en la seÃ±al.

<h1 align="center"><i><b>ğğšğ«ğ­ğ A ğğğ¥ ğ¥ğšğ›ğ¨ğ«ğšğ­ğ¨ğ«ğ¢ğ¨</b></i></h1>


```mermaid
flowchart TD
  A[Inicio - Parte A: PreparaciÃ³n] --> B[Seleccionar hablantes (masculino / femenino)]
  B --> C[Preparar ambiente de grabaciÃ³n: control de ruido, micrÃ³fono, distancia]
  C --> D[Dar instrucciones al sujeto: emitir vocales o frases]
  D --> E[Grabar audio y guardar con etiquetas]
  E --> F[Verificar calidad de la grabaciÃ³n: SNR, clipping]
  F --> G{Â¿Calidad aceptable?}
  G -->|SÃ­| H[Almacenar archivo y metadatos]
  G -->|No| I[Repetir grabaciÃ³n o descartar]
  I --> F
  H --> J[Fin Parte A - Listo para procesamiento]
```

En esta parte del codigo se utiliza la funciÃ³n `wav.read()` de `SciPy` para cargar el archivo  y obtener su frecuencia de muestreo y datos de la seÃ±al. Si el audio tiene mÃ¡s de un canal, se selecciona solo uno para trabajar en mono. Luego, con `np.linspace()` de `NumPy`, se crea el eje de tiempo para cada muestra. La librerÃ­a `Matplotlib (plt.plot())` se usa para graficar la seÃ±al, mostrando la amplitud frente al tiempo. Finalmente, con `Audio()` de `IPython.display`, se reproduce el sonido directamente en el entorno de ejecuciÃ³n.este procedimiento se realiza con cada una de las seÃ±ales tanto de mujeres como para hombres.


```python
#SeÃ±al mujer 1
ratem1, Mujer1 = wav.read("/Mujer1.wav")
if Mujer1.ndim > 1:
    Mujer1 = Mujer1[:,0]

t1 = np.linspace(0, len(Mujer1)/ratem1, num=len(Mujer1))
plt.figure(figsize=(12,4))
plt.plot(t1, Mujer1, color="#FFC1CC")
plt.title("SeÃ±al de audio: Mujer1")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.grid(True)
plt.show()
display(Audio(Mujer1, rate=ratem1))
```
## resultado
*Audio de la seÃ±al*

ğŸ§ [AUDIO MUJER 1](Mujer1.wav)

*seÃ±al generada*
<p align="center">
<img width="800" height="200" alt="image" src="https://github.com/user-attachments/assets/14757bf6-05d2-4da9-b1d2-df0050c1f588" />
</p>

```python
# SeÃ±al mujer 2
ratem2, Mujer2 = wav.read("/Mujer2.wav")
if Mujer2.ndim > 1:
    Mujer2 = Mujer2[:,0]

t2 = np.linspace(0, len(Mujer2)/ratem2, num=len(Mujer2))
plt.figure(figsize=(12,4))
plt.plot(t2, Mujer2, color="#FF69B4")
plt.title("SeÃ±al de audio: Mujer2")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.grid(True)
plt.show()
display(Audio(Mujer2, rate=ratem2))
```
## resultado
*Audio de la seÃ±al*

ğŸ§ [AUDIO MUJER 2](Mujer2.wav)

*seÃ±al generada*
<p align="center">
<img width="800" height="200" alt="image" src="https://github.com/user-attachments/assets/18d09971-acb2-47ad-b98b-75d4d02f7b98" />
</p>

```python
# seÃ±al mujer 3
ratem3, Mujer3 = wav.read("/Mujer3.wav")
if Mujer3.ndim > 1:
    Mujer3 = Mujer3[:,0]

t3 = np.linspace(0, len(Mujer3)/ratem3, num=len(Mujer3))
plt.figure(figsize=(12,4))
plt.plot(t3, Mujer3, color="#FF1493")
plt.title("SeÃ±al de audio: Mujer3")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.grid(True)
plt.show()
display(Audio(Mujer3, rate=ratem3))
```
## resultado
*Audio de la seÃ±al*

ğŸ§ [AUDIO MUJER 3](Mujer3.wav)

*seÃ±al generada*
<p align="center">
<img width="800" height="200" alt="image" src="https://github.com/user-attachments/assets/2fcf033d-8976-4293-91cb-8cbd37fd19aa" />
</p>

```python
# SeÃ±al hombre 1
rateh1, man1 = wav.read("/Man1.wav")
if man1.ndim > 1:
    man1 = man1[:,0]

t1 = np.linspace(0, len(man1)/rateh1, num=len(man1))
plt.figure(figsize=(12,4))
plt.plot(t1, man1, color="#C77DFF")
plt.title("SeÃ±al de audio: hombre 1")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.grid(True)
plt.show()
display(Audio(man1, rate=rateh1))
```
## resultado
*Audio de la seÃ±al*

ğŸ§ [AUDIO HOMBRE 1](Man1.wav)

*seÃ±al generada*
<p align="center">
<img width="800" height="200" alt="image" src="https://github.com/user-attachments/assets/06d82fc7-cf9e-4078-9001-23d8408eeb78" />
</p>

```python
# SeÃ±al hombre 2
rateh2, man2 = wav.read("/Man2.wav")
if man2.ndim > 1:
    man2 = man2[:,0]

t2 = np.linspace(0, len(man2)/rateh2, num=len(man2))
plt.figure(figsize=(12,4))
plt.plot(t2, man2, color="#BA55D3")
plt.title("SeÃ±al de audio: Hombre 2")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.grid(True)
plt.show()
display(Audio(man2, rate=rateh2))
```
## resultado
*Audio de la seÃ±al*

ğŸ§ [AUDIO HOMBRE 2](Man2.wav)

*seÃ±al generada*
<p align="center">
<img width="800" height="200" alt="image" src="https://github.com/user-attachments/assets/aff23ed9-2cb8-4382-a6fc-0966b9f483ae" />
</p>

```python
# seÃ±al hombre 3
rateh3, man3 = wav.read("/man 3.wav")
if man3.ndim > 1:
   man3 = man3[:,0]

t3 = np.linspace(0, len(man3)/rateh3, num=len(man3))
plt.figure(figsize=(12,4))
plt.plot(t3,man3, color="#9400D3")
plt.title("SeÃ±al de audio: Hombre 3")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.grid(True)
plt.show()
display(Audio(man3, rate=rateh3))
```
## resultado
*Audio de la seÃ±al*

ğŸ§ [AUDIO HOMBRE 3](man3.wav)

*seÃ±al generada*
<p align="center">
<img width="800" height="200" alt="image" src="https://github.com/user-attachments/assets/265c2a14-9681-4724-9c95-7e243bd389fe" />
</p>

ğ™©ğ™§ğ™–ğ™£ğ™¨ğ™›ğ™¤ğ™§ğ™¢ğ™–ğ™™ğ™– ğ™™ğ™š ğ™›ğ™¤ğ™ªğ™§ğ™ğ™šğ™§ ğ™® ğ™šğ™¨ğ™¥ğ™šğ™˜ğ™©ğ™§ğ™¤ ğ™™ğ™š ğ™¢ğ™–ğ™œğ™£ğ™ğ™©ğ™ªğ™™

Posteriormente para poder obtener la transformada de fourier y el espectro lo que se realiza es cargar el audio,  conviertirlo a mono y aplicar la Transformada de Fourier (FFT) para obtener sus componentes en frecuencia. Luego se grafican dos resultados: la Transformada de Fourier, que muestra cÃ³mo se distribuyen las frecuencias del sonido, y el espectro de magnitud en decibelios, que indica la intensidad de cada frecuencia presente en la seÃ±al.

```python
#MUJER 1
fs, audio = wavfile.read('/Mujer1.wav')

if audio.ndim > 1:
    audio = np.mean(audio, axis=1)

N = len(audio)
X = np.fft.fft(audio)
f = np.linspace(0, fs/2, N//2)
X_real_pos = np.abs(X.real[:N//2])  # valor absoluto elimina negativos
mag = np.abs(X[:N//2]) / N
plt.figure(figsize=(12, 8))

# Transformada de fourier
plt.subplot(2, 1, 1)
plt.plot(f, X_real_pos,color="#D36CA0")
plt.title('Transformada de Fourier (AUDIO MUJER 1)')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Amplitud  ')
plt.grid(True)

# Espectro de magnitud
plt.subplot(2, 1, 2)
plt.plot(f, 20*np.log10(mag + 1e-12),color="#A94F8A" )
plt.title('Espectro de Magnitud ')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Magnitud [dB]')
plt.grid(True)

plt.tight_layout()
plt.show()
```
## resultado
<p align="center">
<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/8baf1b7f-2378-406d-8f27-380e5ff7939d" />
</p>


```python
#MUJER 2
fs, audio = wavfile.read('/Mujer2.wav')

if audio.ndim > 1:
    audio = np.mean(audio, axis=1)

N = len(audio)
X = np.fft.fft(audio)
f = np.linspace(0, fs/2, N//2)
X_real_pos = np.abs(X.real[:N//2])  # valor absoluto elimina negativos
mag = np.abs(X[:N//2]) / N
plt.figure(figsize=(12, 8))

#Transformada de fourier
plt.subplot(2, 1, 1)
plt.plot(f, X_real_pos,color="#D36CA0")
plt.title('Transformada de Fourier (AUDIO MUJER 2)')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Amplitud ')
plt.grid(True)

# Espectro de magnitud
plt.subplot(2, 1, 2)
plt.plot(f, 20*np.log10(mag + 1e-12),color="#A94F8A")
plt.title('Espectro de Magnitud ')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Magnitud [dB]')
plt.grid(True)

plt.tight_layout()
plt.show()
```
## resultado
<p align="center">
<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/31971d0e-4aa9-4d65-8cf7-89541f3e5d8b" />
    </p>



```python
#MUJER 3
fs, audio = wavfile.read('/Mujer3.wav')

if audio.ndim > 1:
    audio = np.mean(audio, axis=1)

N = len(audio)
X = np.fft.fft(audio)
f = np.linspace(0, fs/2, N//2)
X_real_pos = np.abs(X.real[:N//2])  # valor absoluto elimina negativos
mag = np.abs(X[:N//2]) / N
plt.figure(figsize=(12, 8))

# Transformada de fourier
plt.subplot(2, 1, 1)
plt.plot(f, X_real_pos,color="#D36CA0")
plt.title('Transformada de Fourier (AUDIO MUJER 3)')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Amplitud ')
plt.grid(True)

# Espectro de magnitud
plt.subplot(2, 1, 2)
plt.plot(f, 20*np.log10(mag + 1e-12),color="#A94F8A")
plt.title('Espectro de Magnitud ')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Magnitud [dB]')
plt.grid(True)

plt.tight_layout()
plt.show()
```
## resultado
<p align="center">
<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/bf9e2991-724d-4286-ac5d-5e364dbdac9a" />
</p>

```python
#HOMBRE 1
fs, audio = wavfile.read('/Man1.wav')

if audio.ndim > 1:
    audio = np.mean(audio, axis=1)

N = len(audio)
X = np.fft.fft(audio)
f = np.linspace(0, fs/2, N//2)
X_real_pos = np.abs(X.real[:N//2])  # valor absoluto elimina negativos
mag = np.abs(X[:N//2]) / N
plt.figure(figsize=(12, 8))

#Transformada de fourier
plt.subplot(2, 1, 1)
plt.plot(f, X_real_pos,color="#32CD32")
plt.title('Transformada de Fourier (AUDIO HOMBRE 1)')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Amplitud ')
plt.grid(True)

# Espectro de magnitud
plt.subplot(2, 1, 2)
plt.plot(f, 20*np.log10(mag + 1e-12),color="#00FF7F")
plt.title('Espectro de Magnitud ')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Magnitud [dB]')
plt.grid(True)

plt.tight_layout()
plt.show()
```

## resultado
<p align="center">
<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/56bbd888-96fa-46aa-9b05-9aa0ce4b0600" />
</p>

```python
#HOMBRE 2
fs, audio = wavfile.read('/Man2.wav')

if audio.ndim > 1:
    audio = np.mean(audio, axis=1)

N = len(audio)
X = np.fft.fft(audio)
f = np.linspace(0, fs/2, N//2)
X_real_pos = np.abs(X.real[:N//2])  # valor absoluto elimina negativos
mag = np.abs(X[:N//2]) / N
plt.figure(figsize=(12, 8))

# Transformada de fourier
plt.subplot(2, 1, 1)
plt.plot(f, X_real_pos, color="#32CD32")
plt.title('Transformada de Fourier (AUDIO HOMBRE 2)')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Amplitud ')
plt.grid(True)

# Espectro de magnitud
plt.subplot(2, 1, 2)
plt.plot(f, 20*np.log10(mag + 1e-12), color="#00FF7F")
plt.title('Espectro de Magnitud ')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Magnitud [dB]')
plt.grid(True)

plt.tight_layout()
plt.show()
```
## resultado
<p align="center">
<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/5ec55699-dff0-487f-afa8-c86c511f9444" />
</p>

```python
#HOMBRE 3
fs, audio = wavfile.read('/man 3.wav')
if audio.ndim > 1:
    audio = np.mean(audio, axis=1)

N = len(audio)
X = np.fft.fft(audio)
f = np.linspace(0, fs/2, N//2)
X_real_pos = np.abs(X.real[:N//2])  # valor absoluto elimina negativos
mag = np.abs(X[:N//2]) / N
plt.figure(figsize=(12, 8))

# Transformada de fourier
plt.subplot(2, 1, 1)
plt.plot(f, X_real_pos,color="#32CD32")
plt.title('Transformada de Fourier (AUDIO HOMBRE 3)')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Amplitud ')
plt.grid(True)

# Espectro de magnitud
plt.subplot(2, 1, 2)
plt.plot(f, 20*np.log10(mag + 1e-12),color="#00FF7F")
plt.title('Espectro de Magnitud ')
plt.xlabel('Frecuencia [Hz]')
plt.ylabel('Magnitud [dB]')
plt.grid(True)

plt.tight_layout()
plt.show()
```
## resultado
<p align="center">
<img width="800" height="400" alt="image" src="https://github.com/user-attachments/assets/82225b2f-82d0-4c0e-9541-1eadd3ca4edb" />
</p>

ğ™ğ™™ğ™šğ™£ğ™©ğ™ğ™›ğ™ğ™˜ğ™–ğ™˜ğ™ğ™¤ğ™£ ğ™® ğ™§ğ™šğ™¥ğ™¤ğ™§ğ™©ğ™š ğ™™ğ™š ğ™¡ğ™–ğ™¨ ğ™˜ğ™–ğ™§ğ™–ğ™˜ğ™©ğ™šğ™§ğ™ğ™¨ğ™©ğ™ğ™˜ğ™–ğ™¨ ğ™™ğ™š ğ™˜ğ™–ğ™™ğ™– ğ™¨ğ™šÃ±ğ™–ğ™¡

El cÃ³digo carga los audios, los convierte a mono y los normaliza. Luego calcula su frecuencia fundamental (fâ‚€) con `find_peaks()`, la frecuencia media, el brillo (energÃ­a en frecuencias altas) y la intensidad RMS (energÃ­a total del sonido). Finalmente, muestra estos valores en una tabla para comparar las caracterÃ­sticas de cada audio.

```python
def calcular_caracteristicas(ruta):
    fs, audio = wavfile.read(ruta)
    if audio.ndim > 1:
        audio = np.mean(audio, axis=1)  # convertir a mono
    audio = audio / np.max(np.abs(audio))  # normalizar
    
    N = len(audio)
    fft_vals = np.abs(np.fft.fft(audio)[:N//2])
    freqs = np.fft.fftfreq(N, 1/fs)[:N//2]
    
    peaks, _ = find_peaks(fft_vals, height=np.max(fft_vals)*0.1)
    f0 = freqs[peaks[0]] if len(peaks) > 0 else 0
    
    f_media = np.sum(freqs * fft_vals) / np.sum(fft_vals)
    
    # --- Brillo (energÃ­a por encima de 1500 Hz) ---
    idx_brillo = freqs > 1500
    brillo = np.sum(fft_vals[idx_brillo]) / np.sum(fft_vals)
    
    # --- Intensidad RMS ---
    intensidad = np.sqrt(np.mean(audio**2))
    
    return f0, f_media, brillo, intensidad

archivos = [
    "/Man1.wav", "/Man2.wav", "/man 3.wav",
    "/Mujer1.wav", "/Mujer2.wav", "/Mujer3.wav"
]

print("\nRESULTADOS DE CADA AUDIO:\n")
print(f"{'Archivo':<15} {'F fund (Hz)':<12} {'F media (Hz)':<15} {'Brillo':<10} {'Intensidad':<12}")

for ruta in archivos:
    f0, f_media, brillo, intensidad = calcular_caracteristicas(ruta)
    print(f"{ruta:<15} {f0:<12.2f} {f_media:<15.2f} {brillo:<10.3f} {intensidad:<12.4f}")

```
## resultado

<img width="500" height="194" alt="image" src="https://github.com/user-attachments/assets/72d6d7eb-34e5-4f73-83f7-df2b3f0d429c" />

<h1 align="center"><i><b>ğ™‹ğ™–ğ™§ğ™©ğ™š ğ˜½ ğ™™ğ™šğ™¡ ğ™¡ğ™–ğ™—ğ™¤ğ™§ğ™–ğ™©ğ™¤ğ™§ğ™ğ™¤</b></i></h1>

### Filtro pasabanda hombre

Antes de iniciar el codigo se desarrollÃ³ a mano el filtro pasabanda para poder encontrar el orden necesario y definir los parametros. 

*IMAGEN DE LOS CALCULOS DE SOFI


Primero se importan las librerias, se lee el archivo `/Man1.wav` y guarda la frecuencia de muestreo en `ratem1` y los datos de la seÃ±al en `Man1`.
DespuÃ©s define los parametros del filtro pasabanda como la frecuencia de corte baja y alta basandose en los valores teoricos. Para hombres estÃ¡ el rango de 80-400Hz.

```python
from scipy import signal
from scipy.io import wavfile
ratem1, Man1 = wav.read("/Man1.wav")

f_low = 80
f_high = 400
order = 4
fs = ratem1
nyquist = fs / 2
low = f_low / nyquist
high = f_high / nyquist

b, a = signal.butter(order, [f_low/nyquist, f_high/nyquist], btype='bandpass')
Man1_filtrada = signal.filtfilt(b, a, Man1)
t = np.linspace(0, len(Man1)/fs, len(Man1))
```
Se asigna la frecuencia de muestreo `ratem1` a la variable `fs` y se calculan las frecuencias normalizadas y la de Nyquist.
Se diseÃ±a el filtro pasabanda tipo butterworth que devuelve los coeficientes del filtro en `b` (numerador) y `a` (denominador).
Aplica el filtro a la seÃ±al usando `filtfilt`, que filtra hacia adelante y hacia atrÃ¡s para eliminar el desfase (fase cero).

*CODIGO E IMAGEN DE BODE

### Filtro pasabanda mujer

Se repite el mismo proceso para la seÃ±al de mujer pero con rango de 150-500Hz.
<img width="966" height="1280" alt="image" src="https://github.com/user-attachments/assets/53dc9c1b-3ec4-4072-8996-6583ac933209" />
<img width="1097" height="1210" alt="image" src="https://github.com/user-attachments/assets/4adf867e-bc3f-4c5c-80e6-1dc103d8a0f6" />

```python
from scipy import signal
from scipy.io import wavfile
ratem1, Mujer1 = wav.read("/Mujer1.wav")

f_low = 150
f_high = 500
order = 2
fs = ratem1
nyquist = fs / 2
low = f_low / nyquist
high = f_high / nyquist

b, a = signal.butter(order, [f_low/nyquist, f_high/nyquist], btype='bandpass')
Mujer1_filtrada = signal.filtfilt(b, a, Mujer1)
t = np.linspace(0, len(Mujer1)/fs, len(Mujer1))

plt.figure(figsize=(12,5))
plt.subplot(2,1,1)
plt.plot(t, Mujer1, color='#6A5ACD')
plt.title("SeÃ±al original (voz mujer)")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.subplot(2,1,2)
plt.plot(t, Mujer1_filtrada, color='#9A00FF')
plt.title("SeÃ±al filtrada (pasa banda 150â€“500 Hz, Butterworth orden 2)")
plt.xlabel("Tiempo [s]")
plt.ylabel("Amplitud")
plt.tight_layout()
plt.show()
```
Se grafica la respuesta de frecuencia (Diagrama Bode de magnitud)

## Grafico de respuesta de frecuencia (bode)

```pythom
w, h = signal.freqz(b, a, worN=4096)
f = (w/np.pi) * (fs/2)

plt.figure(figsize=(8,4))
plt.plot(f, 20*np.log10(abs(h)))
plt.title("Respuesta en frecuencia del filtro pasa-banda (zoom 0â€“1000 Hz)")
plt.xlabel("Frecuencia [Hz]")
plt.ylabel("Magnitud [dB]")
plt.xlim(0, 1000)
plt.ylim(-60, 5)
plt.grid(True)
plt.show()
```
<img width="716" height="393" alt="image" src="https://github.com/user-attachments/assets/1629a50d-9b10-4816-bfec-5510a41f71e1" />


## MediciÃ³n jitter y shimmer

Se importan las librerias, se define la funciÃ³n `jitter_shimmer` donde se recibe la seÃ±al de voz, se calculan el jitter y shimmer y sus porcentajes.
Finalmente se visualiza una grafica donde se ve en azul la seÃ±al original de voz, en rojo los cruces por ceros y en verde los picos. Y una tabla con los resultados de cada seÃ±al.

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.io import wavfile
from scipy.signal import find_peaks
import pandas as pd

#FunciÃ³n para calcular jitter y shimmer
def jitter_shimmer(audio, fs, nombre):
    # Normalizar seÃ±al
    audio = audio.astype(float)
    audio = audio - np.mean(audio)
    audio = audio / np.max(np.abs(audio))

    #Cruces por cero
    cruces = np.where(np.diff(np.sign(audio)) > 0)[0]
    if len(cruces) < 2:
        print(f"{nombre}: no hay suficientes cruces por cero\n")
        return np.nan, np.nan, np.nan, np.nan

    Ti = np.diff(cruces) / fs
    jitter_abs = np.mean(np.abs(np.diff(Ti)))
    jitter_rel = (jitter_abs / np.mean(Ti)) * 100

    #Picos
    peaks, _ = find_peaks(audio, height=0)
    if len(peaks) < 2:
        print(f"{nombre}: no hay suficientes picos\n")
        return jitter_abs, jitter_rel, np.nan, np.nan

    Ai = audio[peaks]
    shimmer_abs = np.mean(np.abs(np.diff(Ai)))
    shimmer_rel = (shimmer_abs / np.mean(Ai)) * 100

    #Graficar seÃ±al completa
    t = np.linspace(0, len(audio)/fs, len(audio))
    plt.figure(figsize=(10, 4))
    plt.plot(t, audio, 'b', label="SeÃ±al")
    plt.plot(t[cruces], audio[cruces], 'ro', markersize=3, label="Cruces")
    plt.plot(t[peaks], audio[peaks], 'go', markersize=3, label="Picos")
    plt.title(f"{nombre} - SeÃ±al completa")
    plt.xlabel("Tiempo [s]")
    plt.ylabel("Amplitud")
    plt.legend()
    plt.grid(True)
    plt.show(block=False)

    return jitter_abs, jitter_rel, shimmer_abs, shimmer_rel



archivos = {
    "Mujer1": "/content/Mujer1.wav",
    "Mujer2": "/content/Mujer2.wav",
    "Mujer3": "/content/Mujer3.wav",
    "Hombre1": "/content/Man1.wav",
    "Hombre2": "/content/Man2.wav",
    "Hombre3": "/content/man 3.wav"
}

#Calculos
resultados = []

for nombre, ruta in archivos.items():
    fs, audio = wavfile.read(ruta)
    if audio.ndim > 1:
        audio = audio[:, 0]

    jitter_abs, jitter_rel, shimmer_abs, shimmer_rel = jitter_shimmer(audio, fs, nombre)
    resultados.append([nombre, jitter_abs, jitter_rel, shimmer_abs, shimmer_rel])

#Resultados en tabla
df = pd.DataFrame(resultados, columns=["SeÃ±al", "Jitter_abs (s)", "Jitter_rel (%)", "Shimmer_abs", "Shimmer_rel (%)"])

print("\n Resultados de Jitter y Shimmer \n")
print(df.to_string(index=False, justify='center', formatters={
    "Jitter_abs (s)": "{:.6f}".format,
    "Jitter_rel (%)": "{:.2f}".format,
    "Shimmer_abs": "{:.6f}".format,
    "Shimmer_rel (%)": "{:.2f}".format
}))
```
<img width="866" height="393" alt="image" src="https://github.com/user-attachments/assets/af6e28ad-2e1e-4465-a97e-959a65d207b0" />
<img width="866" height="393" alt="image" src="https://github.com/user-attachments/assets/0f465875-2ab9-41e6-a1d2-b6f984b71353" />
<img width="866" height="393" alt="image" src="https://github.com/user-attachments/assets/03d6e32f-79ab-44f3-accb-9dfc781bd8c4" />
<img width="857" height="393" alt="image" src="https://github.com/user-attachments/assets/bcd84f67-e49e-4635-8c00-7151cd49a1aa" />
<img width="866" height="393" alt="image" src="https://github.com/user-attachments/assets/f631cde4-3cfc-4b4d-a245-fe52b6d5c244" />
<img width="866" height="393" alt="image" src="https://github.com/user-attachments/assets/3cce8e3c-c1ec-46ae-8d92-f53375ecf45e" />
<img width="873" height="248" alt="image" src="https://github.com/user-attachments/assets/f1c021ce-8420-47c8-aa4f-bac3c83b5c64" />

## Jitter y shimmer de la seÃ±al de mujer filtrada

posteriormente calculamos los mismos parametros de jitter y shimmer pero para la seÃ±al filtrada con el proposito de comparar que tanto afectaba el filtrado a estos valores
```python
import numpy as np
import scipy.io.wavfile as wav
import scipy.signal as signal
import pandas as pd

#FunciÃ³n para calcular Jitter y Shimmer
def calcular_jitter_shimmer(signal_data, fs):
    # Cruces por cero para medir periodos
    cruces = np.where(np.diff(np.sign(signal_data)))[0]
    periodos = np.diff(cruces) / fs
    if len(periodos) < 2:
        return np.nan, np.nan, np.nan, np.nan

    #Jitter
    jitter_abs = np.mean(np.abs(np.diff(periodos)))
    jitter_rel = 100 * (jitter_abs / np.mean(periodos))

    #Shimmer
    amplitudes = []
    for i in range(len(cruces) - 1):
        amplitudes.append(np.max(signal_data[cruces[i]:cruces[i+1]]) -
                          np.min(signal_data[cruces[i]:cruces[i+1]]))
    amplitudes = np.array(amplitudes)

    shimmer_abs = np.mean(np.abs(np.diff(amplitudes)))
    shimmer_rel = 100 * (shimmer_abs / np.mean(amplitudes))

    return jitter_abs, jitter_rel, shimmer_abs, shimmer_rel

#Calcular mÃ©tricas
ja, jr, sa, sr = calcular_jitter_shimmer(Mujer1_filtrada, ratem1)

#Mostrar resultados en tabla
df_resultados = pd.DataFrame([{
    "SeÃ±al": "Mujer1_filtrada",
    "Jitter abs [s]": ja,
    "Jitter rel [%]": jr,
    "Shimmer abs": sa,
    "Shimmer rel [%]": sr,
}])

df_resultados.index = ["Resultados"]
print(df_resultados.T)

```
<img width="419" height="168" alt="image" src="https://github.com/user-attachments/assets/34c3dc0a-fc29-4282-be6e-18918bc12a0e" />



<h1 align="center"><i><b>ğ™‹ğ™–ğ™§ğ™©ğ™š ğ˜¾ ğ™™ğ™šğ™¡ ğ™¡ğ™–ğ™—ğ™¤ğ™§ğ™–ğ™©ğ™¤ğ™§ğ™ğ™¤</b></i></h1>
se respondera las siguientes preguntas con respecto a los resultados obtenidos 

- *Â¿QuÃ© diferencias se observan en la frecuencia fundamental?*
 
La frecuencia fundamental (F0) fue significativamente mÃ¡s alta en las voces femeninas que en las masculinas.Esto se debe a que las cuerdas vocales de las mujeres son mÃ¡s cortas y delgadas, lo que hace que vibren a una mayor velocidad, en cambio las voces masculinas presentan una F0 mÃ¡s baja, ya que sus cuerdas vocales son mÃ¡s largas y con mayor masa, lo que genera vibraciones mÃ¡s lentas.Por tanto, los hombres tienden a producir sonidos mÃ¡s graves (bajos) y las mujeres sonidos mÃ¡s agudos (altos).

- *Â¿QuÃ© otras diferencias se notan en tÃ©rminos de brillo, media o intensidad?*

Tambien se puede concluir que las voces masculinas suelen mostrar mayor intensidad (nivel de presiÃ³n sonora o volumen) debido a una mayor capacidad pulmonar y resonadores mÃ¡s amplios, mientras que las voces femeninas presentan un brillo espectral mÃ¡s acentuado, es decir, mÃ¡s energÃ­a en frecuencias altas, lo que las hace sonar mÃ¡s claras o agudas.

En cuanto a la media de amplitud, los hombres tienden a tener valores algo superiores por la potencia de emisiÃ³n, mientras que las mujeres muestran menor amplitud pero mÃ¡s variabilidad en las frecuencias.

- *Importancia clÃ­nica del jitter y shimmer en el anÃ¡lisis de la voz*

El jitter mide la variabilidad en la frecuencia (diferencias entre periodos sucesivos de vibraciÃ³n), El shimmer mide la variabilidad en la amplitud (cambios en la intensidad entre ciclos).Ambos parÃ¡metros son indicadores de estabilidad vocal: en una voz sana, las variaciones son mÃ­nimas, mientras que en voces con alteraciones (nÃ³dulos, pÃ³lipos, disfonÃ­as, parÃ¡lisis, etc.) los valores aumentan.

Por eso, tienen gran valor clÃ­nico, ya que ayudan a detectar disfunciones larÃ­ngeas,permiten cuantificar la severidad de la alteraciÃ³n vocal,Sirven para evaluar la eficacia de terapias o tratamientos.Sin embargo, su interpretaciÃ³n requiere precauciÃ³n, ya que pueden verse afectados por el nivel de intensidad, ruido ambiental o calidad de la grabaciÃ³n.por eso, se recomienda analizarlos junto con otros parÃ¡metros como la relaciÃ³n armÃ³nico-ruido (HNR) o medidas cepstrales.

- *Conclusiones*

La frecuencia fundamental es el parÃ¡metro que mÃ¡s diferencia se muestra en hombres y mujeres, donde las voces femeninas presentan valores mayores de F0.
Las voces masculinas tienden a ser mÃ¡s intensas y con menor brillo, mientras que las femeninas son mÃ¡s agudas y con mayor energÃ­a en las frecuencias altas.
Las diferencias en jitter y shimmer es poca, ya que ambos presentan niveles similares cuando se analizan voces sanas y grabadas en condiciones controladas.
